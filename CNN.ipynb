{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIZ7iBEH71p9"
   },
   "source": [
    "## CNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCEvET3K7l9M"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input,Conv2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "PnNDR6XY8XXq",
    "outputId": "5747b3d9-af5c-4ffb-d325-b368dbfae952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "x_train.shape:  (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "x_train,x_test = x_train/255.0, x_test/255.0        # normlaize train and test data(as the original values were between 0 and 255, we scale them between 0 and 1)\n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kfgpUMb9gLj"
   },
   "source": [
    "x_train and x_test contain 28 x 28 grayscale images which is a 2D input.\n",
    "For a CNN, we need 3D for convolution operation(H x W x C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1lBhr0-u9VIk",
    "outputId": "405bd8f4-1f98-4368-e5ff-c2a61c3934a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# data is only 2D\n",
    "# CNN expects 3D input\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)\n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sr3K1K2l-kd_",
    "outputId": "b7096e21-a2b7-4a9b-bb53-7478d731be2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "# Number of classes\n",
    "\n",
    "K = len(set(y_train))\n",
    "print(\"Number of classes: \", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3qEcJB__OD3"
   },
   "outputs": [],
   "source": [
    "# Build the model using functional API\n",
    "\n",
    "i = Input(x_train[0].shape)\n",
    "x = Conv2D(32, (3,3), activation='relu')(i)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(64,(3,3), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(128,(3,3), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Flatten()(x)                              # to convert the images into a feature vector\n",
    "x = Dropout(0.2)(x)                           # to regularize\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) \n",
    "x = Dense(K,activation='softmax')(x)\n",
    "\n",
    "model = Model(i,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3W6ZUFwskMi"
   },
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2Fo98_hqwnh"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "id": "8Oy3RVZlw1Go",
    "outputId": "b44f7d8d-f4c8-4815-e23c-565bb5e125e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.3017172813415527\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.5152587890625\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.517162561416626\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.8903999924659729\n",
      "Validation acc: 0.9750000238418579\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.478553295135498\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.473681092262268\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.5124603509902954\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.9694333076477051\n",
      "Validation acc: 0.9721999764442444\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.4767999649047852\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.4797557592391968\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.5001823902130127\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.9689666628837585\n",
      "Validation acc: 0.9679999947547913\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 128))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7UG8OzzNEGu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fU3gFkYNEOg"
   },
   "source": [
    "## Task 2: CNN for Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lv4_PRIfLtcm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input,Conv2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3zUOFs-hNWGL",
    "outputId": "47c5c2f4-c9a4-4f35-b12e-a2d165cde2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train,x_test = x_train/255.0, x_test/255.0        \n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2eNliiT4NfIp",
    "outputId": "382a90e2-3f3a-4618-fbe3-3848f7a511af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# data is only 2D\n",
    "# CNN expects 3D input\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)\n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5JUBCmsNNkwO",
    "outputId": "f623533e-8f5f-4839-d5f9-2ca89c9f6f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "# Number of classes\n",
    "\n",
    "K1 = len(set(y_train))\n",
    "print(\"Number of classes: \", K1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-JzJ2vNNsda"
   },
   "outputs": [],
   "source": [
    "# Build the model using functional API\n",
    "\n",
    "i = Input(x_train[0].shape)\n",
    "x = Conv2D(32, (3,3), activation='relu')(i)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(64,(3,3), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(128,(3,3), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Flatten()(x)                              \n",
    "x = Dropout(0.2)(x)                          \n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) \n",
    "x = Dense(K1,activation='softmax')(x)\n",
    "\n",
    "model = Model(i,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjQOtIAcN9QB"
   },
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa-uiuHOcdWq"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "id": "85p2gQsyciUD",
    "outputId": "ee7f1c5c-6e46-4ed7-d59f-43f11419e4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.302354335784912\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.9827063083648682\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.9456875324249268\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.4683666527271271\n",
      "Validation acc: 0.5569999814033508\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.867891550064087\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.9012510776519775\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.8570404052734375\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.5922166705131531\n",
      "Validation acc: 0.6176999807357788\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.8053439855575562\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.8745534420013428\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.8906357288360596\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.6158166527748108\n",
      "Validation acc: 0.6173999905586243\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 128))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyMtt3pUiUaW"
   },
   "source": [
    "## Task 3: CNN for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVzIZvhHPF79"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input,Conv2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "30yrawUYpM1y",
    "outputId": "3338fb2a-2e95-4a8a-cfde-cea2077865af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n",
      "x_train.shape:  (50000, 32, 32, 3)\n",
      "y_train.shape:  (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "(x_train,x_test) = x_train/255.0,x_test/255.0\n",
    "(y_train,y_test) = y_train.flatten(),y_test.flatten()\n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NpuAJgxwp38L",
    "outputId": "2c8a17ec-7f02-4c0a-ad29-f2ff1a95d459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "# Number of classes\n",
    "\n",
    "K2 = len(set(y_train))\n",
    "print(\"Number of classes: \", K2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwFBB5mkqJ0v"
   },
   "outputs": [],
   "source": [
    "# Build the model using functional API\n",
    "\n",
    "i = Input(x_train[0].shape)\n",
    "x = Conv2D(32, (3,3), activation='relu')(i)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(64,(3,3), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(128,(3,3), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Flatten()(x)                              # to convert the images into a feature vector\n",
    "x = Dropout(0.5)(x)                           # to regularize\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) \n",
    "x = Dense(K2,activation='softmax')(x)\n",
    "\n",
    "model = Model(i,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0vKmaYdc2BH"
   },
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 512\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFnyzNvjc2K6"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "B9xVgAb_c2Ij",
    "outputId": "ef4339cd-579b-4dd4-ded9-98b1a29c926f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.3025259971618652\n",
      "Seen so far: 128 samples\n",
      "Training acc over epoch: 0.7085666656494141\n",
      "Validation acc: 0.7771000266075134\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.6703609228134155\n",
      "Seen so far: 128 samples\n",
      "Training acc over epoch: 0.7972166538238525\n",
      "Validation acc: 0.7989000082015991\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.6636629104614258\n",
      "Seen so far: 128 samples\n",
      "Training acc over epoch: 0.8096166849136353\n",
      "Validation acc: 0.8095999956130981\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 128))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aamg_gjEsDSt"
   },
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbtsjQNFwG0X"
   },
   "source": [
    "### (a) increasing your filter sizes up to the input image for fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "meSQTxoKD-8x",
    "outputId": "aa6d71bc-8658-4baa-ad9c-a4e584cb04b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "x_train.shape:  (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train,x_test = x_train/255.0, x_test/255.0        \n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4gtyH3yr_n-k",
    "outputId": "9a896697-ce6f-4287-efb5-9a46c5098efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "# Number of classes\n",
    "\n",
    "K1 = len(set(y_train))\n",
    "print(\"Number of classes: \", K1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_GkDDz-jEVZn",
    "outputId": "0157d9d5-af27-4b28-8707-38eafd0f79ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# data is only 2D\n",
    "# CNN expects 3D input\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "x_test = np.expand_dims(x_test, axis = -1)\n",
    "\n",
    "print(\"x_train.shape: \", x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "th0owcR2D_Bb"
   },
   "outputs": [],
   "source": [
    "# Build the model using functional API\n",
    "\n",
    "i = Input(x_train[0].shape)\n",
    "x = Conv2D(32, (1,1), activation='relu')(i)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(64,(1,1), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Conv2D(128,(1,1), activation='relu')(x)\n",
    "\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "   strides=(1, 1), padding='valid') \n",
    "max_pool_2d(x) \n",
    "\n",
    "x = Flatten()(x)                              # to convert the images into a feature vector\n",
    "x = Dropout(0.5)(x)                           # to regularize\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) \n",
    "x = Dense(K1,activation='softmax')(x)\n",
    "\n",
    "model = Model(i,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozhlt3QGC6bU"
   },
   "source": [
    "when I changed the filter size equal to the input size I got the error as below:\n",
    "\n",
    "ValueError: Negative dimension size caused by subtracting 32 from 28 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](input_1, conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,28,28,1], [32,32,1,32]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IG_336F-EUW0"
   },
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18OP9bsMEeWl"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "XlTC7WUuEer2",
    "outputId": "cf2c7d6c-6db3-41f4-8bae-edac2a2aae85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.3024518489837646\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.75399649143219\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.7760828733444214\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.6803500056266785\n",
      "Validation acc: 0.7401999831199646\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.6880104541778564\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.706146240234375\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.7105101346969604\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.7587166428565979\n",
      "Validation acc: 0.8118000030517578\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.6003992557525635\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.683305025100708\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 400: 1.6812744140625\n",
      "Seen so far: 51328 samples\n",
      "Training acc over epoch: 0.7973833084106445\n",
      "Validation acc: 0.817300021648407\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 128))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyWrk6junLHz"
   },
   "source": [
    "### (b) replace pooling with strided convolutions for CIFAR-10 dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "kvGlMBdREg9e",
    "outputId": "f36fc1e0-6e33-4715-ee9d-8c7e603d4ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (50000, 32, 32, 3)\n",
      "y_train.shape:  (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "(x_train,x_test) = x_train/255.0, x_test/255.0\n",
    "(y_train,y_test) = y_train.flatten(), y_test.flatten()\n",
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-afTDKZmnOGd",
    "outputId": "562a995f-4d0f-4d00-d9b9-24bc3df94450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:a  10\n"
     ]
    }
   ],
   "source": [
    "# Number of classes\n",
    "\n",
    "K2 = len(set(y_train))\n",
    "print(\"Number of classes:a \", K2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXiyr8FRnOL4"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "i = Input(x_train[0].shape)\n",
    "x = Conv2D(32, (3,3), strides = 2, activation='relu')(i)           #filter : (3,3)\n",
    "x = Conv2D(64, (3,3), strides=2, activation='relu')(x)             # I have used a stride of 2 so that the image dimensions get reduced by half after each convolution\n",
    "x = Conv2D(128, (3,3), strides=2, activation='relu')(x)\n",
    "x = Flatten()(x)                                                   # Flatten layer to convert the image into a feature vector\n",
    "x = Dropout(0.2)(x)                                                # for regularization\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(K2,activation='softmax')(x)\n",
    "\n",
    "model = Model(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s7HLRfrnODX"
   },
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWJ7kzBFnxdT"
   },
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwUU7Trsnx7P"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "2WsJYAC8nznt",
    "outputId": "3ff442da-8add-458c-9123-ed325920bfac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.3028151988983154\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 2.0910136699676514\n",
      "Seen so far: 25728 samples\n",
      "Training acc over epoch: 0.3396399915218353\n",
      "Validation acc: 0.4374000132083893\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.0169994831085205\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 2.0345849990844727\n",
      "Seen so far: 25728 samples\n",
      "Training acc over epoch: 0.43320000171661377\n",
      "Validation acc: 0.4636000096797943\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.9440230131149292\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 200: 1.9988384246826172\n",
      "Seen so far: 25728 samples\n",
      "Training acc over epoch: 0.47968000173568726\n",
      "Validation acc: 0.4984000027179718\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 128))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yp53rVOeYdCh"
   },
   "source": [
    "Using strides = 2 in our model has lowered the accuracy in this model. Since during the training some of the features got lost in the striding process, hence our model doesn't genralize well of the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "017yEb0EnzyC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IDL_Assignment _3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
