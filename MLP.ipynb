{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8pyDguz3Rlj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import MNISTDataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3guDJQnvpTi"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
    "                    test_images.reshape([-1, 784]), test_labels,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-lOYzQpvv4k"
   },
   "outputs": [],
   "source": [
    "train_steps = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "n_input = 784\n",
    "h1 = 512\n",
    "h2 = 128\n",
    "n_classes = 10\n",
    "\n",
    "# weights and bias initializations\n",
    "W1 = tf.Variable(tf.random.uniform(shape = (n_input,h1), minval = -(math.sqrt(6)/math.sqrt(n_input+h1)),  \n",
    "                            maxval = (math.sqrt(6)/math.sqrt(n_input+h1)))) # Xavier uniform\n",
    "W2 = tf.Variable(tf.random.uniform(shape = (h1,h2), minval = -(math.sqrt(6)/math.sqrt(h1+h2)),\n",
    "                             maxval = (math.sqrt(6)/math.sqrt(h1+h2)))) \n",
    "out = tf.Variable(tf.random.uniform(shape = (h2,n_classes), minval = -(math.sqrt(6/(h2+n_classes))),\n",
    "                                   maxval = math.sqrt(6/(h2+n_classes)) ))\n",
    "\n",
    "b1 = tf.Variable(tf.random.uniform([h1]))\n",
    "b2 = tf.Variable(tf.random.uniform([h2]))\n",
    "b_out = tf.Variable(tf.random.uniform([n_classes]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "FxF_yXDlv1YW",
    "outputId": "8c466f0c-5a44-432e-8513-efa799c2ce8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7673261165618896 Accuracy: 0.125\n",
      "Loss: 2.2043304443359375 Accuracy: 0.1484375\n",
      "Loss: 1.8858424425125122 Accuracy: 0.5234375\n",
      "Loss: 1.376692295074463 Accuracy: 0.6875\n",
      "Loss: 1.0477204322814941 Accuracy: 0.734375\n",
      "Starting new epoch...\n",
      "Loss: 0.7562645673751831 Accuracy: 0.828125\n",
      "Loss: 0.6474988460540771 Accuracy: 0.8671875\n",
      "Loss: 0.6458619832992554 Accuracy: 0.875\n",
      "Loss: 0.6489923000335693 Accuracy: 0.8046875\n",
      "Loss: 0.4788413643836975 Accuracy: 0.8671875\n",
      "Starting new epoch...\n"
     ]
    }
   ],
   "source": [
    "for step in range(train_steps):\n",
    "    img_batch, lbl_batch = data.next_batch()\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit1 = tf.nn.sigmoid(tf.matmul(img_batch, W1) + b1)\n",
    "        logit2 = tf.nn.sigmoid(tf.matmul(logit1, W2) + b2)\n",
    "        output = tf.matmul(logit2,out) + b_out\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=output, labels=lbl_batch))\n",
    "    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n",
    "    \n",
    "    W1.assign_sub(learning_rate * grads[0])\n",
    "    b1.assign_sub(learning_rate * grads[1])\n",
    "    W2.assign_sub(learning_rate * grads[2])\n",
    "    b2.assign_sub(learning_rate * grads[3])\n",
    "    out.assign_sub(learning_rate * grads[4])\n",
    "    b_out.assign_sub(learning_rate * grads[5])\n",
    "\n",
    "        \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
    "                             tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MLVBe5tIv5lv",
    "outputId": "cb31a89e-4980-422b-c17a-d2c94e5c52ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7957, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_preds1 = tf.matmul(data.test_data, W1) + b1\n",
    "test_preds2 = tf.matmul(test_preds1, W2) + b2\n",
    "test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n",
    "                       output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n",
    "                             tf.float32))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCc3iTOKB679"
   },
   "source": [
    "We used the simple softmax on the outer layer and sigmoid in the hidden layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyPViQmTCOWI"
   },
   "source": [
    "The accuracy comes out to be 80.37 % which is worse than the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T86U6wy8Cgbg"
   },
   "source": [
    "We'll try adjusting the activation functions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aV3RLcA2CakL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "LLnAUvRqDDa_",
    "outputId": "7f34bf88-333f-42e1-e6f9-1c0cc15e959f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6021804809570312 Accuracy: 0.796875\n",
      "Loss: 0.24951386451721191 Accuracy: 0.921875\n",
      "Loss: 0.19135567545890808 Accuracy: 0.9296875\n",
      "Loss: 0.32472795248031616 Accuracy: 0.9375\n",
      "Loss: 0.13926321268081665 Accuracy: 0.9609375\n",
      "Starting new epoch...\n",
      "Loss: 0.15404875576496124 Accuracy: 0.9453125\n",
      "Loss: 0.31452053785324097 Accuracy: 0.921875\n",
      "Loss: 0.18411535024642944 Accuracy: 0.9375\n",
      "Loss: 0.08679554611444473 Accuracy: 0.9609375\n",
      "Starting new epoch...\n",
      "Loss: 0.13405629992485046 Accuracy: 0.9609375\n"
     ]
    }
   ],
   "source": [
    "for step in range(train_steps):\n",
    "    img_batch, lbl_batch = data.next_batch()\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit1 = tf.nn.relu(tf.matmul(img_batch, W1) + b1)\n",
    "        logit2 = tf.nn.relu(tf.matmul(logit1, W2) + b2)\n",
    "        output = tf.matmul(logit2,out) + b_out\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=output, labels=lbl_batch))\n",
    "    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n",
    "    \n",
    "    W1.assign_sub(learning_rate * grads[0])\n",
    "    b1.assign_sub(learning_rate * grads[1])\n",
    "    W2.assign_sub(learning_rate * grads[2])\n",
    "    b2.assign_sub(learning_rate * grads[3])\n",
    "    out.assign_sub(learning_rate * grads[4])\n",
    "    b_out.assign_sub(learning_rate * grads[5])\n",
    "\n",
    "        \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
    "                             tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mVbIOrDDJ74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "apjqXh8vDLA9",
    "outputId": "9179b0d8-06bb-4690-fc02-80a1d24557a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8061, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_preds1 = tf.matmul(data.test_data, W1) + b1\n",
    "test_preds2 = tf.matmul(test_preds1, W2) + b2\n",
    "test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n",
    "                       output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n",
    "                             tf.float32))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sesLFbXBD_sY"
   },
   "source": [
    "Using RELU on the hidden layers and Softmax on the Outer layer we observed that the training accuracy increased significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRc0i38mBF7o"
   },
   "source": [
    "## Adjusting weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkhAxYLGEM7b"
   },
   "outputs": [],
   "source": [
    "train_steps = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "n_input = 784\n",
    "h1 = 512\n",
    "h2 = 128\n",
    "n_classes = 10\n",
    "\n",
    "# weights and bias initializations\n",
    "W1 = tf.Variable(tf.random.uniform(shape = (n_input,h1), minval = -0.1,maxval = 0.1)) # Xavier uniform\n",
    "W2 = tf.Variable(tf.random.uniform(shape = (h1,h2), minval = -0.1,maxval = 0.1)) \n",
    "out = tf.Variable(tf.random.uniform(shape = (h2,n_classes), minval = -0.1,maxval =0.2))\n",
    "\n",
    "b1 = tf.Variable(tf.random.uniform([h1]))\n",
    "b2 = tf.Variable(tf.random.uniform([h2]))\n",
    "b_out = tf.Variable(tf.random.uniform([n_classes]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "dnFZL-aTBwwK",
    "outputId": "0457c9fa-1a35-4a5a-d1dd-9370604d7af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.309579372406006 Accuracy: 0.1484375\n",
      "Loss: 2.1901614665985107 Accuracy: 0.28125\n",
      "Loss: 1.9276642799377441 Accuracy: 0.4140625\n",
      "Loss: 1.4804365634918213 Accuracy: 0.640625\n",
      "Starting new epoch...\n",
      "Loss: 1.254069209098816 Accuracy: 0.6875\n",
      "Loss: 0.877936840057373 Accuracy: 0.78125\n",
      "Loss: 0.7249398231506348 Accuracy: 0.84375\n",
      "Loss: 0.6332966685295105 Accuracy: 0.8359375\n",
      "Loss: 0.541893720626831 Accuracy: 0.8671875\n",
      "Starting new epoch...\n",
      "Loss: 0.5570704340934753 Accuracy: 0.8359375\n"
     ]
    }
   ],
   "source": [
    "for step in range(train_steps):\n",
    "    img_batch, lbl_batch = data.next_batch()\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit1 = tf.nn.sigmoid(tf.matmul(img_batch, W1) + b1)\n",
    "        logit2 = tf.nn.sigmoid(tf.matmul(logit1, W2) + b2)\n",
    "        output = tf.matmul(logit2,out) + b_out\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=output, labels=lbl_batch))\n",
    "    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n",
    "    \n",
    "    W1.assign_sub(learning_rate * grads[0])\n",
    "    b1.assign_sub(learning_rate * grads[1])\n",
    "    W2.assign_sub(learning_rate * grads[2])\n",
    "    b2.assign_sub(learning_rate * grads[3])\n",
    "    out.assign_sub(learning_rate * grads[4])\n",
    "    b_out.assign_sub(learning_rate * grads[5])\n",
    "\n",
    "        \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
    "                             tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_3HOEJcDDMM8",
    "outputId": "769ba46a-d1eb-4d32-fa4c-42bf57a5bfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.098, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_preds1 = tf.matmul(data.test_data, W1) + b1\n",
    "test_preds2 = tf.matmul(test_preds1, W2) + b2\n",
    "test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n",
    "                       output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n",
    "                             tf.float32))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzvNk5cJBR1I"
   },
   "source": [
    "Adjust weights and biases gives loss of 0.55 and Accuracy of 83%\n",
    "\n",
    "\n",
    "Now adjusting the activation function (relu) on the changed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "jByCXanuDYlO",
    "outputId": "2a55171d-8373-42b2-ea09-34eff718e306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2685776948928833 Accuracy: 0.796875\n",
      "Loss: 0.24316102266311646 Accuracy: 0.8984375\n",
      "Loss: 0.19263552129268646 Accuracy: 0.9453125\n",
      "Loss: 0.312516450881958 Accuracy: 0.90625\n",
      "Starting new epoch...\n",
      "Loss: 0.3839570879936218 Accuracy: 0.890625\n",
      "Loss: 0.23785501718521118 Accuracy: 0.9296875\n",
      "Loss: 0.22646576166152954 Accuracy: 0.921875\n",
      "Loss: 0.11145719885826111 Accuracy: 0.9609375\n",
      "Starting new epoch...\n",
      "Loss: 0.17879125475883484 Accuracy: 0.9296875\n",
      "Loss: 0.1379394680261612 Accuracy: 0.953125\n"
     ]
    }
   ],
   "source": [
    "for step in range(train_steps):\n",
    "    img_batch, lbl_batch = data.next_batch()\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit1 = tf.nn.relu(tf.matmul(img_batch, W1) + b1)\n",
    "        logit2 = tf.nn.relu(tf.matmul(logit1, W2) + b2)\n",
    "        output = tf.matmul(logit2,out) + b_out\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=output, labels=lbl_batch))\n",
    "    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n",
    "    \n",
    "    W1.assign_sub(learning_rate * grads[0])\n",
    "    b1.assign_sub(learning_rate * grads[1])\n",
    "    W2.assign_sub(learning_rate * grads[2])\n",
    "    b2.assign_sub(learning_rate * grads[3])\n",
    "    out.assign_sub(learning_rate * grads[4])\n",
    "    b_out.assign_sub(learning_rate * grads[5])\n",
    "\n",
    "        \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
    "                             tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ip0s2_pB_Eh"
   },
   "source": [
    "After adjusting the activatio function we see improvement in loss and accuracy.\n",
    "We get loss of 0.13 and accuracy of 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o7BL2NFqDh_V",
    "outputId": "dc06d8c5-b918-4173-877a-d7a2dc298837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8328, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_preds1 = tf.matmul(data.test_data, W1) + b1\n",
    "test_preds2 = tf.matmul(test_preds1, W2) + b2\n",
    "test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n",
    "                       output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n",
    "                             tf.float32))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzivuIryD-kQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4QCw5bvJzMY"
   },
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UsTXaQN9DoEe"
   },
   "outputs": [],
   "source": [
    "tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
    "                    test_images.reshape([-1, 784]), test_labels,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUfiSVE8La91"
   },
   "outputs": [],
   "source": [
    "train_steps = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "n_input = 784\n",
    "h1 = 512\n",
    "h2 = 128\n",
    "n_classes = 10\n",
    "\n",
    "# weights and bias initializations\n",
    "W1 = tf.Variable(tf.random.uniform(shape = (n_input,h1), minval = -(math.sqrt(6)/math.sqrt(n_input+h1)),  \n",
    "                            maxval = (math.sqrt(6)/math.sqrt(n_input+h1)))) # Xavier uniform\n",
    "W2 = tf.Variable(tf.random.uniform(shape = (h1,h2), minval = -(math.sqrt(6)/math.sqrt(h1+h2)),\n",
    "                             maxval = (math.sqrt(6)/math.sqrt(h1+h2)))) \n",
    "out = tf.Variable(tf.random.uniform(shape = (h2,n_classes), minval = -(math.sqrt(6/(h2+n_classes))),\n",
    "                                   maxval = math.sqrt(6/(h2+n_classes)) ))\n",
    "\n",
    "b1 = tf.Variable(tf.random.uniform([h1]))\n",
    "b2 = tf.Variable(tf.random.uniform([h2]))\n",
    "b_out = tf.Variable(tf.random.uniform([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "YJble3FaMbwe",
    "outputId": "f8520c24-da86-4af0-8585-ecce8cd349da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.687591075897217 Accuracy: 0.0859375\n",
      "Loss: 2.206146240234375 Accuracy: 0.3125\n",
      "Loss: 1.8497662544250488 Accuracy: 0.5\n",
      "Loss: 1.419837474822998 Accuracy: 0.6796875\n",
      "Loss: 1.0113914012908936 Accuracy: 0.7265625\n",
      "Starting new epoch...\n",
      "Loss: 0.7953387498855591 Accuracy: 0.8125\n",
      "Loss: 0.7797795534133911 Accuracy: 0.8046875\n",
      "Loss: 0.550279438495636 Accuracy: 0.8515625\n",
      "Loss: 0.536864161491394 Accuracy: 0.8203125\n",
      "Loss: 0.5461123585700989 Accuracy: 0.890625\n",
      "Starting new epoch...\n"
     ]
    }
   ],
   "source": [
    "for step in range(train_steps):\n",
    "    img_batch, lbl_batch = data.next_batch()\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit1 = tf.nn.sigmoid(tf.matmul(img_batch, W1) + b1)\n",
    "        logit2 = tf.nn.sigmoid(tf.matmul(logit1, W2) + b2)\n",
    "        output = tf.matmul(logit2,out) + b_out\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=output, labels=lbl_batch))\n",
    "    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n",
    "    \n",
    "    W1.assign_sub(learning_rate * grads[0])\n",
    "    b1.assign_sub(learning_rate * grads[1])\n",
    "    W2.assign_sub(learning_rate * grads[2])\n",
    "    b2.assign_sub(learning_rate * grads[3])\n",
    "    out.assign_sub(learning_rate * grads[4])\n",
    "    b_out.assign_sub(learning_rate * grads[5])\n",
    "\n",
    "        \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
    "                             tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZBEqq1c1Mfoe",
    "outputId": "dae87e57-f1c8-4008-ed8d-b3a51ac4d7a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8166, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_preds1 = tf.matmul(data.test_data, W1) + b1\n",
    "test_preds2 = tf.matmul(test_preds1, W2) + b2\n",
    "test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n",
    "                       output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n",
    "                             tf.float32))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R-E0t5RCoWH"
   },
   "source": [
    "Adjusting activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "wdfFb7n0MmKu",
    "outputId": "91ae42fd-19a9-4d04-a9fe-ed753709b492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1053563356399536 Accuracy: 0.8671875\n",
      "Loss: 0.5295026302337646 Accuracy: 0.8671875\n",
      "Loss: 0.24275965988636017 Accuracy: 0.9296875\n",
      "Loss: 0.33690667152404785 Accuracy: 0.9140625\n",
      "Loss: 0.2732604146003723 Accuracy: 0.9140625\n",
      "Starting new epoch...\n",
      "Loss: 0.14272314310073853 Accuracy: 0.9453125\n",
      "Loss: 0.19466015696525574 Accuracy: 0.9453125\n",
      "Loss: 0.13944914937019348 Accuracy: 0.96875\n",
      "Loss: 0.163396418094635 Accuracy: 0.9609375\n",
      "Starting new epoch...\n",
      "Loss: 0.19348980486392975 Accuracy: 0.9609375\n"
     ]
    }
   ],
   "source": [
    "for step in range(train_steps):\n",
    "    img_batch, lbl_batch = data.next_batch()\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        logit1 = tf.nn.relu(tf.matmul(img_batch, W1) + b1)\n",
    "        logit2 = tf.nn.relu(tf.matmul(logit1, W2) + b2)\n",
    "        output = tf.matmul(logit2,out) + b_out\n",
    "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=output, labels=lbl_batch))\n",
    "    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n",
    "    \n",
    "    W1.assign_sub(learning_rate * grads[0])\n",
    "    b1.assign_sub(learning_rate * grads[1])\n",
    "    W2.assign_sub(learning_rate * grads[2])\n",
    "    b2.assign_sub(learning_rate * grads[3])\n",
    "    out.assign_sub(learning_rate * grads[4])\n",
    "    b_out.assign_sub(learning_rate * grads[5])\n",
    "\n",
    "        \n",
    "    if not step % 100:\n",
    "        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
    "                             tf.float32))\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUfCMmBDCw7y"
   },
   "source": [
    "Adjusting the activation function gives us better loss (0.193) and accuracy (96%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qci4weRkMuzR",
    "outputId": "d3f87537-bd03-436d-ed98-4acf8c2e5a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8411, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_preds1 = tf.matmul(data.test_data, W1) + b1\n",
    "test_preds2 = tf.matmul(test_preds1, W2) + b2\n",
    "test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n",
    "                       output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n",
    "                             tf.float32))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfGE7knyM3gT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
